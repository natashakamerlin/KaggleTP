{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Tabular Playground Series March 2021\n\nThe aim is to predict a binary target based on a number of categorical (cat0-cat18) and continuous (cont0-cont10) features given in a tabular dataset (train.csv). \n\nThis notebook will use a neural network. "},{"metadata":{},"cell_type":"markdown","source":"### Import all the necessary libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import mutual_info_classif\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom sklearn.model_selection import GridSearchCV\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom numpy.random import seed\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load the data\n\nDo some basic checks for the data shape and potentially missing values. We will assume that there is no risk of target leakage (i.e., dataset variables created/updated after the target value is realized)."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/tabular-playground-series-mar-2021/train.csv')","execution_count":3,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"# Check for missing data\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have no missing data. Next, separate target column from predictors and create the different training and validation sets before performing any data preprocessing steps, to avoid train-test contamination."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Separate target from predictors\nX=train.copy()\nID=X.pop('id')\ny=X.pop('target')\n\n#Break off validation set from training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess categorical features\n\nMachine learning models can only interpret numerical data and not text data, so we will need to turn the categorical features in our dataset into numerical data by feature encoding. This dataset contains 19 categorical features (cat0-cat18). "},{"metadata":{},"cell_type":"markdown","source":"Check the cardinality of the features. For high-cardinality features, one-hot encoding would generate many features. We will instead use target encoding for categorical features with more than 50 unique values. For categorical features with only 2 unique values, we will use label encoding. For the rest, we will use one-hot encoding."},{"metadata":{"trusted":false},"cell_type":"code","source":"cat_columns = ['cat' + str(i) for i in range(19)]\ntrain_X[cat_columns].nunique().sort_values()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Label encode:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"le_cols = ['cat0', 'cat11', 'cat12', 'cat13', 'cat14']\n\nlabel_encoder = LabelEncoder()\nfor col in le_cols:\n    train_X[col] = label_encoder.fit_transform(train_X[col])\n    val_X[col] = label_encoder.transform(val_X[col])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**One-hot encode:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"oh_cols = ['cat1', 'cat2', 'cat3', 'cat4', 'cat6', 'cat9', 'cat15', 'cat16',\n           'cat17', 'cat18']\n\nonehot = OneHotEncoder(handle_unknown = 'ignore', sparse=False)\noh_train_X = pd.DataFrame(onehot.fit_transform(train_X[oh_cols]))\noh_val_X = pd.DataFrame(onehot.transform(val_X[oh_cols]))\n\noh_train_X.index = train_X.index\noh_val_X.index = val_X.index\n\noh_train_X.columns = onehot.get_feature_names()\noh_val_X.columns = onehot.get_feature_names()\n\nnum_train_X = train_X.drop(oh_cols, axis=1)\nnum_val_X = val_X.drop(oh_cols, axis=1)\n\ntrain_X = pd.concat([oh_train_X, num_train_X], axis=1)\nval_X = pd.concat([oh_val_X, num_val_X], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Target encode:**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Break off 'encoding' set from training data\nX_encode, pretrain_X_trimmed, y_encode, train_y_trimmed = train_test_split(train_X, train_y, train_size=0.2, random_state=0)\n\nencoder = MEstimateEncoder(cols=['cat5', 'cat7', 'cat8', 'cat10'], m=7.0) \n\n# Fit the encoder on the encoding split\nencoder.fit(X_encode, y_encode)\n\n# Encode to create the final training data\ntrain_X_trimmed = encoder.transform(pretrain_X_trimmed)\n\nval_X = encoder.transform(val_X)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Preprocess numerical features\nNeural networks generally perform better with scaled features, with values that are not too far from 0. Unscaled variables can produce slow or unstable learning processes. Standardize the continuous features."},{"metadata":{"trusted":false},"cell_type":"code","source":"cont_columns = ['cont' + str(i) for i in range(11)]\n\nscaler=StandardScaler()\ntrain_X_scaled = pd.DataFrame(scaler.fit_transform(train_X_trimmed[cont_columns]), \n                              index=train_X_trimmed.index, columns=cont_columns)\nval_X_scaled = pd.DataFrame(scaler.transform(val_X[cont_columns]), \n                            index=val_X.index, columns=cont_columns)\n\ntrain_X_trimmed.drop(cont_columns, axis=1, inplace=True)\nval_X.drop(cont_columns, axis=1, inplace=True)\n\ntrain_X_scaled = pd.concat([train_X_scaled, train_X_trimmed], axis=1)\nval_X_scaled = pd.concat([val_X_scaled, val_X], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Select subset of features using mutual information\n\nUse mutual information (MI) to select a subset of features for training our neural network. If the MI score is high for a given feature then it is a strong indicator of the target. If MI=0, the feature is independent of the target. The feature may still contribute an interaction effect to other features even though its MI score is low. Here, we will simply remove features with low MI scores. "},{"metadata":{"trusted":false},"cell_type":"code","source":"def make_mi_scores(X, y):\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(train_X_scaled, train_y_trimmed)\nmi_scores[::3]  # Show a few features with their MI scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Drop columns with MI scores less than 0.0002\nmi_scores=pd.DataFrame(mi_scores)\ncols_new=mi_scores[mi_scores['MI Scores'].gt(0.0002)].index\n\nnew_train_X=train_X_scaled[cols_new]\nnew_val_X=val_X_scaled[cols_new]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Build the neural network\n\nThe model will be built with 2 hidden layers and tuned using GridSearch to find the optimum number of hidden nodes and dropout rate. In addition to the dropout layer, a batch normalization layer is added. For the hidden layers, the activation function is set to *ReLU* and the optimization algorithm to *Adam*. The output layer instead uses a *Sigmoid* activation function and *binary cross-entropy* loss function, since this is a binary classification problem.\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"def create_model(nodes=100, act='relu', opt='Adam', dr=0.0):\n\n    # Set random seed\n    seed(0)\n    tf.random.set_seed(0)\n    \n    model = Sequential()\n    \n    model.add(Dense(nodes, activation=act, input_dim=new_train_X.shape[1]))\n    model.add(Dropout(dr))       # Add dropout, default none (dr=0.0)\n    model.add(BatchNormalization())         # Add batch normalization\n    model.add(Dense(nodes, activation=act))\n    model.add(BatchNormalization())         # Add batch normalization\n    model.add(Dense(1, activation='sigmoid'))        # Output layer\n    \n    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n\n    return model\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model = create_model()\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will test the neural network here, using the default values given in the model along with early stopping to avoid overfitting. "},{"metadata":{"trusted":false},"cell_type":"code","source":"training = model.fit(new_train_X, train_y_trimmed, \n                     validation_data=(new_val_X, val_y),\n                     epochs=1000, batch_size=32, \n                     callbacks=[early_stopping], verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"val_acc = np.mean(training.history['val_binary_accuracy'])\nprint(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.plot(training.history['binary_accuracy'])\nplt.plot(training.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the grid search parameters. Ideally, more parameters would be tested, but this is omitted to save some time."},{"metadata":{"scrolled":true,"trusted":false},"cell_type":"code","source":"model = KerasClassifier(build_fn=create_model, verbose=0)\n\nnodes=[25, 50]\ndrops = [0.0, 0.1]\nparam_grid = dict(batch_size=[32], epochs=[100], nodes=nodes,dr=drops)\n\n# Search the grid\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\ngrid_result = grid.fit(new_train_X, train_y_trimmed, \n                       validation_data=(new_val_X, val_y), callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print the best results of GridSearchCV."},{"metadata":{"trusted":false},"cell_type":"code","source":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, fit the model using the optimal GridSearchCV values and evaluate the area under the ROC curve between the predicted probability and the observed target."},{"metadata":{"trusted":false},"cell_type":"code","source":"model = create_model(nodes=50, dr=0.1)\n\ntraining = model.fit(new_train_X, train_y_trimmed, epochs=100, batch_size=32, validation_data=(new_val_X, val_y), callbacks=[early_stopping])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"preds= model.predict(new_val_X)\nfalse_positive_rate, recall, thresholds = roc_curve(val_y, preds)\nroc_auc = auc(false_positive_rate, recall)\nprint(roc_auc)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}